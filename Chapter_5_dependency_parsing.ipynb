{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 5: Dependency parsing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5: Dependency parsing\n",
        "https://nlp100.github.io/en/ch05.html The zip archive ai.en.zip contains the text of the Wikipedia article, “Artificial Intelligence”. Apply a dependency parser to the text, and store the result in a file. Implement programs that read the dependency trees and perform the jobs.\n",
        "\n",
        "For your convenience, the zip archive also includes ai.en.txt.json, the text with dependency trees predicted by Stanford CoreNLP and stored in JSON format."
      ],
      "metadata": {
        "id": "HFuM1VYhYX1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "## 40. Read the parse result (words)\n",
        "\n",
        "Design a class Word that represents a word. This class has three member variables, text (word surface), lemma (lemma), and pos (part-of-speech). Represent a sentence as an array of instances of Word class. Implement a program to load the parse result, and store the text as an array of sentences. Show the object of the first sentence of the body of the article."
      ],
      "metadata": {
        "id": "1Sb1gRoCYeiZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P55J1SkzXo_1",
        "outputId": "e147b6e5-6976-475f-ffb6-b7b11fb2332f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "basePath = \"/content/drive/MyDrive/nlp100/Dataset/ai-en/\"\n",
        "rawtext = \"ai.en.txt\"\n",
        "jsn = 'ai.en.txt.json'\n",
        "\n",
        "f = open(basePath + rawtext, 'r', encoding = \"ISO-8859-1\")\n",
        "aiRaw = f.read()\n",
        "aiRaw = re.sub(r\"\\n\", r\" \", aiRaw)\n",
        "aiRaw = aiRaw.strip()\n",
        "aiRawPerSentence = aiRaw.split(\".\")\n",
        "\n",
        "f = open(basePath + jsn, 'r')\n",
        "aiJson = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aiRaw[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Yk_dm5iPZUNe",
        "outputId": "9fe0d716-fbff-4460-ec69-98b3cb85accc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Ai:\n",
        "  def __init__(self, pathJsn):\n",
        "    self._sentences = []\n",
        "    self._aiJson = []\n",
        "\n",
        "    f = open(basePath + jsn, 'r')\n",
        "    self._aiJson = json.load(f)\n",
        "\n",
        "    currentSentence = []\n",
        "\n",
        "    for i, sentence in enumerate(self._aiJson['sentences']):\n",
        "      for j, word in enumerate(sentence['tokens']):\n",
        "        a = self.Word(\n",
        "            text = word['word'],\n",
        "            lemma = word['lemma'],\n",
        "            pos = word['pos'],\n",
        "        )\n",
        "        \n",
        "        currentSentence.append(a)\n",
        "\n",
        "      self._sentences.append(currentSentence)\n",
        "      currentSentence = []\n",
        "\n",
        "  class Word:\n",
        "    def __init__(self, text, lemma, pos):\n",
        "      self._text = text\n",
        "      self._lemma = lemma\n",
        "      self._pos = pos"
      ],
      "metadata": {
        "id": "gp0kzBAgZpTo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai = Ai(basePath + jsn)\n",
        "\n",
        "for index, word in enumerate(ai._sentences[0]):\n",
        "  print(\"==== index\",index,\"====\")\n",
        "  print(\"text\\t:\", word._text)\n",
        "  print(\"lemma\\t:\",word._lemma)\n",
        "  print(\"pos\\t:\", word._pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk6JxvuLlkMm",
        "outputId": "bd0d41fd-30d5-4c3e-aa66-ce4b996266dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== index 0 ====\n",
            "text\t: In\n",
            "lemma\t: in\n",
            "pos\t: IN\n",
            "==== index 1 ====\n",
            "text\t: computer\n",
            "lemma\t: computer\n",
            "pos\t: NN\n",
            "==== index 2 ====\n",
            "text\t: science\n",
            "lemma\t: science\n",
            "pos\t: NN\n",
            "==== index 3 ====\n",
            "text\t: ,\n",
            "lemma\t: ,\n",
            "pos\t: ,\n",
            "==== index 4 ====\n",
            "text\t: artificial\n",
            "lemma\t: artificial\n",
            "pos\t: JJ\n",
            "==== index 5 ====\n",
            "text\t: intelligence\n",
            "lemma\t: intelligence\n",
            "pos\t: NN\n",
            "==== index 6 ====\n",
            "text\t: -LRB-\n",
            "lemma\t: -lrb-\n",
            "pos\t: -LRB-\n",
            "==== index 7 ====\n",
            "text\t: AI\n",
            "lemma\t: ai\n",
            "pos\t: NN\n",
            "==== index 8 ====\n",
            "text\t: -RRB-\n",
            "lemma\t: -rrb-\n",
            "pos\t: -RRB-\n",
            "==== index 9 ====\n",
            "text\t: ,\n",
            "lemma\t: ,\n",
            "pos\t: ,\n",
            "==== index 10 ====\n",
            "text\t: sometimes\n",
            "lemma\t: sometimes\n",
            "pos\t: RB\n",
            "==== index 11 ====\n",
            "text\t: called\n",
            "lemma\t: call\n",
            "pos\t: VBN\n",
            "==== index 12 ====\n",
            "text\t: machine\n",
            "lemma\t: machine\n",
            "pos\t: NN\n",
            "==== index 13 ====\n",
            "text\t: intelligence\n",
            "lemma\t: intelligence\n",
            "pos\t: NN\n",
            "==== index 14 ====\n",
            "text\t: ,\n",
            "lemma\t: ,\n",
            "pos\t: ,\n",
            "==== index 15 ====\n",
            "text\t: is\n",
            "lemma\t: be\n",
            "pos\t: VBZ\n",
            "==== index 16 ====\n",
            "text\t: intelligence\n",
            "lemma\t: intelligence\n",
            "pos\t: NN\n",
            "==== index 17 ====\n",
            "text\t: demonstrated\n",
            "lemma\t: demonstrate\n",
            "pos\t: VBN\n",
            "==== index 18 ====\n",
            "text\t: by\n",
            "lemma\t: by\n",
            "pos\t: IN\n",
            "==== index 19 ====\n",
            "text\t: machines\n",
            "lemma\t: machine\n",
            "pos\t: NNS\n",
            "==== index 20 ====\n",
            "text\t: ,\n",
            "lemma\t: ,\n",
            "pos\t: ,\n",
            "==== index 21 ====\n",
            "text\t: in\n",
            "lemma\t: in\n",
            "pos\t: IN\n",
            "==== index 22 ====\n",
            "text\t: contrast\n",
            "lemma\t: contrast\n",
            "pos\t: NN\n",
            "==== index 23 ====\n",
            "text\t: to\n",
            "lemma\t: to\n",
            "pos\t: TO\n",
            "==== index 24 ====\n",
            "text\t: the\n",
            "lemma\t: the\n",
            "pos\t: DT\n",
            "==== index 25 ====\n",
            "text\t: natural\n",
            "lemma\t: natural\n",
            "pos\t: JJ\n",
            "==== index 26 ====\n",
            "text\t: intelligence\n",
            "lemma\t: intelligence\n",
            "pos\t: NN\n",
            "==== index 27 ====\n",
            "text\t: displayed\n",
            "lemma\t: display\n",
            "pos\t: VBN\n",
            "==== index 28 ====\n",
            "text\t: by\n",
            "lemma\t: by\n",
            "pos\t: IN\n",
            "==== index 29 ====\n",
            "text\t: humans\n",
            "lemma\t: human\n",
            "pos\t: NNS\n",
            "==== index 30 ====\n",
            "text\t: and\n",
            "lemma\t: and\n",
            "pos\t: CC\n",
            "==== index 31 ====\n",
            "text\t: animals\n",
            "lemma\t: animal\n",
            "pos\t: NNS\n",
            "==== index 32 ====\n",
            "text\t: .\n",
            "lemma\t: .\n",
            "pos\t: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "## 41. Read the parse result (dependency)\n",
        "\n",
        "In addition to problem 40, add three member variables head (a reference to the object of its syntactic governor), dep (dependency type to its governor), and children (a list of references to the syntactic dependents in the parse tree) to the class Word. Show the pairs of governors (parents) and their dependents (children) of the first sentence of the body of the article. Use the class Word in the rest of the problems in this chapter."
      ],
      "metadata": {
        "id": "1Av20h77IJ5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ai:\n",
        "  def __init__(self, pathJsn):\n",
        "    self._sentences = []\n",
        "    self._aiJson = []\n",
        "\n",
        "    f = open(basePath + jsn, 'r')\n",
        "    self._aiJson = json.load(f)\n",
        "\n",
        "    currentSentence = []\n",
        "\n",
        "    for i, sentence in enumerate(self._aiJson['sentences']):\n",
        "      for j, word in enumerate(sentence['tokens']):\n",
        "        for k, dic in enumerate(sentence['basicDependencies']):\n",
        "          if dic['dependentGloss'] == word['word']:\n",
        "\n",
        "            a = self.Word(\n",
        "                id = j+1,\n",
        "                text = word['word'],\n",
        "                lemma = word['lemma'],\n",
        "                pos = word['pos'],\n",
        "                head = dic['governorGloss'],\n",
        "                headId = dic['governor'],\n",
        "                dep = dic['dep'],\n",
        "            )\n",
        "        \n",
        "        currentSentence.append(a)\n",
        "\n",
        "      self._sentences.append(currentSentence)\n",
        "      currentSentence = []\n",
        "\n",
        "  class Word:\n",
        "    def __init__(self, id, text, lemma, pos, head, headId, dep):\n",
        "      self._id = id\n",
        "      self._text = text\n",
        "      self._lemma = lemma\n",
        "      self._pos = pos\n",
        "\n",
        "      self._head = head\n",
        "      self._headId = headId\n",
        "      self._dep = dep\n"
      ],
      "metadata": {
        "id": "mfPHLOPgWhyv"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ai = Ai(basePath + jsn)\n",
        "\n",
        "for index, word in enumerate(ai._sentences[0]):\n",
        "  print(\"==== index\",index,\"====\")\n",
        "  print(\"id\\t:\", word._id)\n",
        "  print(\"text\\t:\", word._text)\n",
        "  print(\"lemma\\t:\",word._lemma)\n",
        "  print(\"pos\\t:\", word._pos)\n",
        "  print(\"head\\t:\", word._head)\n",
        "  print(\"dep\\t:\", word._dep)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8ozcrI6ht4N",
        "outputId": "8bcb17d8-9f64-43e5-95aa-3785db1b75ae"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== index 0 ====\n",
            "id\t: 1\n",
            "text\t: In\n",
            "lemma\t: in\n",
            "pos\t: IN\n",
            "head\t: science\n",
            "dep\t: case\n",
            "==== index 1 ====\n",
            "id\t: 2\n",
            "text\t: computer\n",
            "lemma\t: computer\n",
            "pos\t: NN\n",
            "head\t: science\n",
            "dep\t: compound\n",
            "==== index 2 ====\n",
            "id\t: 3\n",
            "text\t: science\n",
            "lemma\t: science\n",
            "pos\t: NN\n",
            "head\t: called\n",
            "dep\t: nmod\n",
            "==== index 3 ====\n",
            "id\t: 4\n",
            "text\t: ,\n",
            "lemma\t: ,\n",
            "pos\t: ,\n",
            "head\t: intelligence\n",
            "dep\t: punct\n",
            "==== index 4 ====\n",
            "id\t: 5\n",
            "text\t: artificial\n",
            "lemma\t: artificial\n",
            "pos\t: JJ\n",
            "head\t: intelligence\n",
            "dep\t: amod\n",
            "==== index 5 ====\n",
            "id\t: 6\n",
            "text\t: intelligence\n",
            "lemma\t: intelligence\n",
            "pos\t: NN\n",
            "head\t: contrast\n",
            "dep\t: nmod\n",
            "==== index 6 ====\n",
            "id\t: 7\n",
            "text\t: -LRB-\n",
            "lemma\t: -lrb-\n",
            "pos\t: -LRB-\n",
            "head\t: AI\n",
            "dep\t: punct\n",
            "==== index 7 ====\n",
            "id\t: 8\n",
            "text\t: AI\n",
            "lemma\t: ai\n",
            "pos\t: NN\n",
            "head\t: intelligence\n",
            "dep\t: appos\n",
            "==== index 8 ====\n",
            "id\t: 9\n",
            "text\t: -RRB-\n",
            "lemma\t: -rrb-\n",
            "pos\t: -RRB-\n",
            "head\t: AI\n",
            "dep\t: punct\n",
            "==== index 9 ====\n",
            "id\t: 10\n",
            "text\t: ,\n",
            "lemma\t: ,\n",
            "pos\t: ,\n",
            "head\t: intelligence\n",
            "dep\t: punct\n",
            "==== index 10 ====\n",
            "id\t: 11\n",
            "text\t: sometimes\n",
            "lemma\t: sometimes\n",
            "pos\t: RB\n",
            "head\t: called\n",
            "dep\t: advmod\n",
            "==== index 11 ====\n",
            "id\t: 12\n",
            "text\t: called\n",
            "lemma\t: call\n",
            "pos\t: VBN\n",
            "head\t: ROOT\n",
            "dep\t: ROOT\n",
            "==== index 12 ====\n",
            "id\t: 13\n",
            "text\t: machine\n",
            "lemma\t: machine\n",
            "pos\t: NN\n",
            "head\t: intelligence\n",
            "dep\t: compound\n",
            "==== index 13 ====\n",
            "id\t: 14\n",
            "text\t: intelligence\n",
            "lemma\t: intelligence\n",
            "pos\t: NN\n",
            "head\t: contrast\n",
            "dep\t: nmod\n",
            "==== index 14 ====\n",
            "id\t: 15\n",
            "text\t: ,\n",
            "lemma\t: ,\n",
            "pos\t: ,\n",
            "head\t: intelligence\n",
            "dep\t: punct\n",
            "==== index 15 ====\n",
            "id\t: 16\n",
            "text\t: is\n",
            "lemma\t: be\n",
            "pos\t: VBZ\n",
            "head\t: called\n",
            "dep\t: advcl\n",
            "==== index 16 ====\n",
            "id\t: 17\n",
            "text\t: intelligence\n",
            "lemma\t: intelligence\n",
            "pos\t: NN\n",
            "head\t: contrast\n",
            "dep\t: nmod\n",
            "==== index 17 ====\n",
            "id\t: 18\n",
            "text\t: demonstrated\n",
            "lemma\t: demonstrate\n",
            "pos\t: VBN\n",
            "head\t: intelligence\n",
            "dep\t: acl\n",
            "==== index 18 ====\n",
            "id\t: 19\n",
            "text\t: by\n",
            "lemma\t: by\n",
            "pos\t: IN\n",
            "head\t: humans\n",
            "dep\t: case\n",
            "==== index 19 ====\n",
            "id\t: 20\n",
            "text\t: machines\n",
            "lemma\t: machine\n",
            "pos\t: NNS\n",
            "head\t: demonstrated\n",
            "dep\t: nmod\n",
            "==== index 20 ====\n",
            "id\t: 21\n",
            "text\t: ,\n",
            "lemma\t: ,\n",
            "pos\t: ,\n",
            "head\t: intelligence\n",
            "dep\t: punct\n",
            "==== index 21 ====\n",
            "id\t: 22\n",
            "text\t: in\n",
            "lemma\t: in\n",
            "pos\t: IN\n",
            "head\t: contrast\n",
            "dep\t: case\n",
            "==== index 22 ====\n",
            "id\t: 23\n",
            "text\t: contrast\n",
            "lemma\t: contrast\n",
            "pos\t: NN\n",
            "head\t: intelligence\n",
            "dep\t: nmod\n",
            "==== index 23 ====\n",
            "id\t: 24\n",
            "text\t: to\n",
            "lemma\t: to\n",
            "pos\t: TO\n",
            "head\t: intelligence\n",
            "dep\t: case\n",
            "==== index 24 ====\n",
            "id\t: 25\n",
            "text\t: the\n",
            "lemma\t: the\n",
            "pos\t: DT\n",
            "head\t: intelligence\n",
            "dep\t: det\n",
            "==== index 25 ====\n",
            "id\t: 26\n",
            "text\t: natural\n",
            "lemma\t: natural\n",
            "pos\t: JJ\n",
            "head\t: intelligence\n",
            "dep\t: amod\n",
            "==== index 26 ====\n",
            "id\t: 27\n",
            "text\t: intelligence\n",
            "lemma\t: intelligence\n",
            "pos\t: NN\n",
            "head\t: contrast\n",
            "dep\t: nmod\n",
            "==== index 27 ====\n",
            "id\t: 28\n",
            "text\t: displayed\n",
            "lemma\t: display\n",
            "pos\t: VBN\n",
            "head\t: intelligence\n",
            "dep\t: acl\n",
            "==== index 28 ====\n",
            "id\t: 29\n",
            "text\t: by\n",
            "lemma\t: by\n",
            "pos\t: IN\n",
            "head\t: humans\n",
            "dep\t: case\n",
            "==== index 29 ====\n",
            "id\t: 30\n",
            "text\t: humans\n",
            "lemma\t: human\n",
            "pos\t: NNS\n",
            "head\t: displayed\n",
            "dep\t: nmod\n",
            "==== index 30 ====\n",
            "id\t: 31\n",
            "text\t: and\n",
            "lemma\t: and\n",
            "pos\t: CC\n",
            "head\t: humans\n",
            "dep\t: cc\n",
            "==== index 31 ====\n",
            "id\t: 32\n",
            "text\t: animals\n",
            "lemma\t: animal\n",
            "pos\t: NNS\n",
            "head\t: humans\n",
            "dep\t: conj\n",
            "==== index 32 ====\n",
            "id\t: 33\n",
            "text\t: .\n",
            "lemma\t: .\n",
            "pos\t: .\n",
            "head\t: called\n",
            "dep\t: punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "## 42. Show root words\n",
        "For each sentence, extract the root word (whose head is ROOT)."
      ],
      "metadata": {
        "id": "LWiPgpl5IOIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences[:4]):\n",
        "  for j, word in enumerate(sentence):\n",
        "    if word._head == 'ROOT':\n",
        "      print(word._text,\"is the root of sentence \",i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIk-X0XPik7A",
        "outputId": "25bc6ba3-2ac0-404e-d691-9fb99f8a2628"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "called is the root of sentence  0\n",
            "define is the root of sentence  1\n",
            "used is the root of sentence  2\n",
            "removed is the root of sentence  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "## 43. Show verb governors and noun dependents\n",
        "Show all pairs of verb governors (parents) and their noun dependents (children) from all sentences in the text."
      ],
      "metadata": {
        "id": "KRuhENlxIXFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences[:1]):\n",
        "  print(\"======= sentence\",i,\"=======\")\n",
        "  for j, word in enumerate(sentence):\n",
        "    print(\"parent\\t: \",word._head,\" \\t\\tchild\\t: \",word._text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hue5s-ckdiq",
        "outputId": "3edb341c-4dec-41b8-8bac-12e5e66b3fef"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= sentence 0 =======\n",
            "parent\t:  science  \t\tchild\t:  In\n",
            "parent\t:  science  \t\tchild\t:  computer\n",
            "parent\t:  called  \t\tchild\t:  science\n",
            "parent\t:  intelligence  \t\tchild\t:  ,\n",
            "parent\t:  intelligence  \t\tchild\t:  artificial\n",
            "parent\t:  contrast  \t\tchild\t:  intelligence\n",
            "parent\t:  AI  \t\tchild\t:  -LRB-\n",
            "parent\t:  intelligence  \t\tchild\t:  AI\n",
            "parent\t:  AI  \t\tchild\t:  -RRB-\n",
            "parent\t:  intelligence  \t\tchild\t:  ,\n",
            "parent\t:  called  \t\tchild\t:  sometimes\n",
            "parent\t:  ROOT  \t\tchild\t:  called\n",
            "parent\t:  intelligence  \t\tchild\t:  machine\n",
            "parent\t:  contrast  \t\tchild\t:  intelligence\n",
            "parent\t:  intelligence  \t\tchild\t:  ,\n",
            "parent\t:  called  \t\tchild\t:  is\n",
            "parent\t:  contrast  \t\tchild\t:  intelligence\n",
            "parent\t:  intelligence  \t\tchild\t:  demonstrated\n",
            "parent\t:  humans  \t\tchild\t:  by\n",
            "parent\t:  demonstrated  \t\tchild\t:  machines\n",
            "parent\t:  intelligence  \t\tchild\t:  ,\n",
            "parent\t:  contrast  \t\tchild\t:  in\n",
            "parent\t:  intelligence  \t\tchild\t:  contrast\n",
            "parent\t:  intelligence  \t\tchild\t:  to\n",
            "parent\t:  intelligence  \t\tchild\t:  the\n",
            "parent\t:  intelligence  \t\tchild\t:  natural\n",
            "parent\t:  contrast  \t\tchild\t:  intelligence\n",
            "parent\t:  intelligence  \t\tchild\t:  displayed\n",
            "parent\t:  humans  \t\tchild\t:  by\n",
            "parent\t:  displayed  \t\tchild\t:  humans\n",
            "parent\t:  humans  \t\tchild\t:  and\n",
            "parent\t:  humans  \t\tchild\t:  animals\n",
            "parent\t:  called  \t\tchild\t:  .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "## 44. Visualize dependency trees\n",
        "Visualize a dependency tree of a sentence as a directed graph. Consider converting a dependency tree into DOT language and use Graphviz for drawing a directed graph. In addition, you can use pydot for drawing a dependency tree."
      ],
      "metadata": {
        "id": "L9vpUEsAIcu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph"
      ],
      "metadata": {
        "id": "KY4uS-YX1Uv4"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot = Digraph(node_attr={'shape': 'plaintext'})\n",
        "dot.attr(rankdir='LR')\n",
        "dot.node(str(0), \"ROOT\")\n",
        "for  word in ai._sentences[2]:\n",
        "  dot.node(str(word._id), word._text)\n",
        "\n",
        "for word in ai._sentences[2]:\n",
        "  dot.edge(str(word._headId), str(word._id))\n",
        "\n",
        "dot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C2lISSsq8VRe",
        "outputId": "7fb8ec11-cf97-4e70-f2f5-775fff4184aa"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7fc1acc4a750>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"920pt\" height=\"989pt\"\n viewBox=\"0.00 0.00 920.00 989.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 985)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-985 916,-985 916,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<text text-anchor=\"middle\" x=\"28\" y=\"-797.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ROOT</text>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<text text-anchor=\"middle\" x=\"119\" y=\"-797.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">used</text>\n</g>\n<!-- 0&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>0&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M56.2941,-801C64.3803,-801 73.3168,-801 81.8479,-801\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"81.9684,-804.5001 91.9684,-801 81.9683,-797.5001 81.9684,-804.5001\"/>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<text text-anchor=\"middle\" x=\"344.5\" y=\"-959.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Colloquially</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<text text-anchor=\"middle\" x=\"678\" y=\"-770.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">,</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<text text-anchor=\"middle\" x=\"879.5\" y=\"-689.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">the</text>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<text text-anchor=\"middle\" x=\"344.5\" y=\"-905.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">term</text>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-554.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">``</text>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<text text-anchor=\"middle\" x=\"344.5\" y=\"-851.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">artificial</text>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<text text-anchor=\"middle\" x=\"223\" y=\"-905.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">intelligence</text>\n</g>\n<!-- 7&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>7&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M263.7957,-927.1314C273.6132,-931.4947 284.2127,-936.2056 294.3998,-940.7333\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"293.0455,-943.9614 303.6051,-944.8245 295.8885,-937.5647 293.0455,-943.9614\"/>\n</g>\n<!-- 7&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>7&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M264.1344,-909C277.9322,-909 293.2423,-909 306.7665,-909\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"307.1668,-912.5001 317.1668,-909 307.1667,-905.5001 307.1668,-912.5001\"/>\n</g>\n<!-- 7&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>7&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M263.7957,-890.8686C276.7768,-885.0992 291.1254,-878.7221 304.0798,-872.9645\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"305.5101,-876.159 313.2268,-868.8992 302.6671,-869.7623 305.5101,-876.159\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-500.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">&#39;&#39;</text>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<text text-anchor=\"middle\" x=\"223\" y=\"-851.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">is</text>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<text text-anchor=\"middle\" x=\"223\" y=\"-797.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">often</text>\n</g>\n<!-- 11&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>11&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M130.5407,-819.123C142.1756,-836.5179 161.3288,-862.8946 182,-882 182.9506,-882.8786 183.9327,-883.7479 184.9392,-884.6061\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"182.9854,-887.5215 193.0163,-890.9321 187.3016,-882.0106 182.9854,-887.5215\"/>\n</g>\n<!-- 11&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>11&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M146.0533,-815.0469C158.474,-821.4961 173.3833,-829.2375 186.757,-836.1815\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"185.233,-839.3339 195.7209,-840.8358 188.4588,-833.1214 185.233,-839.3339\"/>\n</g>\n<!-- 11&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>11&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M146.0533,-801C158.1157,-801 172.5252,-801 185.5959,-801\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"185.7209,-804.5001 195.7209,-801 185.7208,-797.5001 185.7209,-804.5001\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<text text-anchor=\"middle\" x=\"223\" y=\"-743.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">describe</text>\n</g>\n<!-- 11&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>11&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M146.0533,-786.9531C157.0841,-781.2256 170.0777,-774.4789 182.2153,-768.1767\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"184.0213,-771.1827 191.2834,-763.4682 180.7956,-764.9702 184.0213,-771.1827\"/>\n</g>\n<!-- 43 -->\n<g id=\"node44\" class=\"node\">\n<title>43</title>\n<text text-anchor=\"middle\" x=\"223\" y=\"-689.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">.</text>\n</g>\n<!-- 11&#45;&gt;43 -->\n<g id=\"edge43\" class=\"edge\">\n<title>11&#45;&gt;43</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M130.5407,-782.877C142.1756,-765.4821 161.3288,-739.1054 182,-720 183.6782,-718.4489 185.4546,-716.9266 187.2902,-715.444\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"189.6845,-718.024 195.5968,-709.2321 185.4923,-712.4181 189.6845,-718.024\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<text text-anchor=\"middle\" x=\"344.5\" y=\"-770.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">to</text>\n</g>\n<!-- 13&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>13&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M254.6055,-754.0234C270.6763,-757.5947 290.3131,-761.9585 307.1296,-765.6955\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"306.6575,-769.1759 317.1786,-767.9286 308.176,-762.3425 306.6575,-769.1759\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<text text-anchor=\"middle\" x=\"344.5\" y=\"-716.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">machines</text>\n</g>\n<!-- 13&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>13&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M254.6055,-739.9766C268.2022,-736.9551 284.3514,-733.3664 299.1856,-730.0699\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"300.1952,-733.431 309.1978,-727.8449 298.6766,-726.5977 300.1952,-733.431\"/>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<text text-anchor=\"middle\" x=\"463\" y=\"-756.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">computers</text>\n</g>\n<!-- 14&#45;&gt;17 -->\n<g id=\"edge17\" class=\"edge\">\n<title>14&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M379.7331,-731.893C390.8194,-735.6353 403.2562,-739.8333 415.0694,-743.8209\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"414.2083,-747.2242 424.8025,-747.1063 416.4471,-740.5919 414.2083,-747.2242\"/>\n</g>\n<!-- 20 -->\n<g id=\"node21\" class=\"node\">\n<title>20</title>\n<text text-anchor=\"middle\" x=\"463\" y=\"-689.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mimic</text>\n</g>\n<!-- 14&#45;&gt;20 -->\n<g id=\"edge20\" class=\"edge\">\n<title>14&#45;&gt;20</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M379.7331,-711.9722C394.3671,-708.6379 411.354,-704.7674 426.1538,-701.3953\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"427.0221,-704.7873 435.9947,-699.1531 425.467,-697.9622 427.0221,-704.7873\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<text text-anchor=\"middle\" x=\"572\" y=\"-824.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">&#45;LRB&#45;</text>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<text text-anchor=\"middle\" x=\"572\" y=\"-770.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">or</text>\n</g>\n<!-- 17&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>17&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M491.9238,-778.0442C505.5036,-786.516 521.8231,-796.697 536.1766,-805.6515\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"534.4806,-808.7186 544.8175,-811.0421 538.1857,-802.7796 534.4806,-808.7186\"/>\n</g>\n<!-- 17&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>17&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M501.1229,-764.8965C512.0094,-766.2948 523.8569,-767.8165 534.6939,-769.2084\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"534.3456,-772.6923 544.71,-770.4949 535.2374,-765.7494 534.3456,-772.6923\"/>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<text text-anchor=\"middle\" x=\"572\" y=\"-716.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">&#45;RRB&#45;</text>\n</g>\n<!-- 17&#45;&gt;18 -->\n<g id=\"edge18\" class=\"edge\">\n<title>17&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M501.1229,-746.0099C512.0276,-742.0082 523.8966,-737.6526 534.7483,-733.6703\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"536.1894,-736.8698 544.3714,-730.1389 533.7778,-730.2983 536.1894,-736.8698\"/>\n</g>\n<!-- 19 -->\n<g id=\"node20\" class=\"node\">\n<title>19</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-770.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">that</text>\n</g>\n<!-- 24 -->\n<g id=\"node25\" class=\"node\">\n<title>24</title>\n<text text-anchor=\"middle\" x=\"572\" y=\"-662.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">functions</text>\n</g>\n<!-- 20&#45;&gt;24 -->\n<g id=\"edge24\" class=\"edge\">\n<title>20&#45;&gt;24</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M490.2241,-686.2564C501.3694,-683.4957 514.5651,-680.227 527.0674,-677.1301\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"528.0279,-680.498 536.8929,-674.6962 526.3447,-673.7034 528.0279,-680.498\"/>\n</g>\n<!-- 21 -->\n<g id=\"node22\" class=\"node\">\n<title>21</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">``</text>\n</g>\n<!-- 22 -->\n<g id=\"node23\" class=\"node\">\n<title>22</title>\n<text text-anchor=\"middle\" x=\"678\" y=\"-716.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">cognitive</text>\n</g>\n<!-- 23 -->\n<g id=\"node24\" class=\"node\">\n<title>23</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-392.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">&#39;&#39;</text>\n</g>\n<!-- 24&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>24&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M597.7038,-684.2436C600.9807,-687.0173 604.1726,-689.9702 607,-693 626.6797,-714.0882 623.3203,-725.9118 643,-747 643.6185,-747.6628 644.2544,-748.3219 644.9049,-748.9762\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"642.5611,-751.5757 652.2962,-755.7564 647.293,-746.4173 642.5611,-751.5757\"/>\n</g>\n<!-- 24&#45;&gt;22 -->\n<g id=\"edge22\" class=\"edge\">\n<title>24&#45;&gt;22</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M607.0021,-683.8313C615.5059,-688.1634 624.7111,-692.8528 633.5863,-697.3741\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"632.1733,-700.5823 642.6725,-702.003 635.3508,-694.345 632.1733,-700.5823\"/>\n</g>\n<!-- 27 -->\n<g id=\"node28\" class=\"node\">\n<title>27</title>\n<text text-anchor=\"middle\" x=\"678\" y=\"-662.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">associate</text>\n</g>\n<!-- 24&#45;&gt;27 -->\n<g id=\"edge27\" class=\"edge\">\n<title>24&#45;&gt;27</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M607.0021,-666C615.8526,-666 625.4629,-666 634.6702,-666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"634.8324,-669.5001 644.8323,-666 634.8323,-662.5001 634.8324,-669.5001\"/>\n</g>\n<!-- 32 -->\n<g id=\"node33\" class=\"node\">\n<title>32</title>\n<text text-anchor=\"middle\" x=\"678\" y=\"-608.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">,</text>\n</g>\n<!-- 24&#45;&gt;32 -->\n<g id=\"edge32\" class=\"edge\">\n<title>24&#45;&gt;32</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M607.0021,-648.1687C618.1206,-642.5046 630.4382,-636.2296 641.6495,-630.5182\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"643.4345,-633.5369 650.7562,-625.8789 640.257,-627.2996 643.4345,-633.5369\"/>\n</g>\n<!-- 36 -->\n<g id=\"node37\" class=\"node\">\n<title>36</title>\n<text text-anchor=\"middle\" x=\"678\" y=\"-311.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">learning</text>\n</g>\n<!-- 24&#45;&gt;36 -->\n<g id=\"edge36\" class=\"edge\">\n<title>24&#45;&gt;36</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M577.5498,-647.6229C594.9726,-589.9303 648.6606,-412.152 669.5558,-342.9614\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"672.9647,-343.78 672.5052,-333.1952 666.2636,-341.7563 672.9647,-343.78\"/>\n</g>\n<!-- 25 -->\n<g id=\"node26\" class=\"node\">\n<title>25</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-716.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">that</text>\n</g>\n<!-- 26 -->\n<g id=\"node27\" class=\"node\">\n<title>26</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-662.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">humans</text>\n</g>\n<!-- 27&#45;&gt;19 -->\n<g id=\"edge19\" class=\"edge\">\n<title>27&#45;&gt;19</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M703.7038,-684.2436C706.9807,-687.0173 710.1726,-689.9702 713,-693 732.6797,-714.0882 729.956,-725.3361 749,-747 749.4948,-747.5628 750.0011,-748.1249 750.5172,-748.6854\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"748.089,-751.2069 757.633,-755.8072 753.0408,-746.2593 748.089,-751.2069\"/>\n</g>\n<!-- 27&#45;&gt;25 -->\n<g id=\"edge25\" class=\"edge\">\n<title>27&#45;&gt;25</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M711.1165,-683.5323C721.5545,-689.0582 733.1368,-695.1901 743.7793,-700.8243\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"742.362,-704.0342 752.8375,-705.6199 745.6372,-697.8477 742.362,-704.0342\"/>\n</g>\n<!-- 27&#45;&gt;26 -->\n<g id=\"edge26\" class=\"edge\">\n<title>27&#45;&gt;26</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M711.1165,-666C719.9075,-666 729.5102,-666 738.6796,-666\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"738.7817,-669.5001 748.7816,-666 738.7816,-662.5001 738.7817,-669.5001\"/>\n</g>\n<!-- 31 -->\n<g id=\"node32\" class=\"node\">\n<title>31</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-608.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mind</text>\n</g>\n<!-- 27&#45;&gt;31 -->\n<g id=\"edge31\" class=\"edge\">\n<title>27&#45;&gt;31</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M711.1165,-648.4677C721.5545,-642.9418 733.1368,-636.8099 743.7793,-631.1757\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"745.6372,-634.1523 752.8375,-626.3801 742.362,-627.9658 745.6372,-634.1523\"/>\n</g>\n<!-- 28 -->\n<g id=\"node29\" class=\"node\">\n<title>28</title>\n<text text-anchor=\"middle\" x=\"879.5\" y=\"-635.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">with</text>\n</g>\n<!-- 29 -->\n<g id=\"node30\" class=\"node\">\n<title>29</title>\n<text text-anchor=\"middle\" x=\"879.5\" y=\"-581.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">the</text>\n</g>\n<!-- 30 -->\n<g id=\"node31\" class=\"node\">\n<title>30</title>\n<text text-anchor=\"middle\" x=\"879.5\" y=\"-527.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">human</text>\n</g>\n<!-- 31&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>31&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M802.5781,-630.3802C816.4143,-641.6438 834.2953,-656.2002 849.3348,-668.4434\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"847.3892,-671.3726 857.354,-674.9716 851.8085,-665.944 847.3892,-671.3726\"/>\n</g>\n<!-- 31&#45;&gt;28 -->\n<g id=\"edge28\" class=\"edge\">\n<title>31&#45;&gt;28</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M807.1888,-619.3779C818.0476,-622.3245 830.7256,-625.7647 842.4065,-628.9344\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"841.7764,-632.39 852.3441,-631.6311 843.6097,-625.6343 841.7764,-632.39\"/>\n</g>\n<!-- 31&#45;&gt;29 -->\n<g id=\"edge29\" class=\"edge\">\n<title>31&#45;&gt;29</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M807.1888,-604.6221C818.0476,-601.6755 830.7256,-598.2353 842.4065,-595.0656\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"843.6097,-598.3657 852.3441,-592.3689 841.7764,-591.61 843.6097,-598.3657\"/>\n</g>\n<!-- 31&#45;&gt;30 -->\n<g id=\"edge30\" class=\"edge\">\n<title>31&#45;&gt;30</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M802.5781,-593.6198C816.4143,-582.3562 834.2953,-567.7998 849.3348,-555.5566\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"851.8085,-558.056 857.354,-549.0284 847.3892,-552.6274 851.8085,-558.056\"/>\n</g>\n<!-- 33 -->\n<g id=\"node34\" class=\"node\">\n<title>33</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-338.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">such</text>\n</g>\n<!-- 34 -->\n<g id=\"node35\" class=\"node\">\n<title>34</title>\n<text text-anchor=\"middle\" x=\"879.5\" y=\"-338.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">as</text>\n</g>\n<!-- 33&#45;&gt;34 -->\n<g id=\"edge34\" class=\"edge\">\n<title>33&#45;&gt;34</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M807.1888,-342C817.9379,-342 830.4696,-342 842.0522,-342\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"842.3441,-345.5001 852.3441,-342 842.344,-338.5001 842.3441,-345.5001\"/>\n</g>\n<!-- 35 -->\n<g id=\"node36\" class=\"node\">\n<title>35</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-284.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">``</text>\n</g>\n<!-- 36&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>36&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M680.6267,-333.21C686.9439,-372.2792 705.7813,-465.878 749,-531 749.1924,-531.2899 749.3887,-531.5785 749.5887,-531.8659\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"747.119,-534.3678 756.2271,-539.7803 752.4823,-529.8694 747.119,-534.3678\"/>\n</g>\n<!-- 36&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>36&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M683.132,-333.3707C692.5501,-364.917 714.8182,-430.477 749,-477 749.3566,-477.4853 749.7233,-477.9679 750.0991,-478.4475\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"747.5222,-480.8162 756.8185,-485.8985 752.7206,-476.1282 747.5222,-480.8162\"/>\n</g>\n<!-- 36&#45;&gt;21 -->\n<g id=\"edge21\" class=\"edge\">\n<title>36&#45;&gt;21</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M687.9364,-333.2545C700.5598,-355.6209 723.7595,-394.1001 749,-423 749.493,-423.5644 749.9976,-424.128 750.5122,-424.6898\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"748.079,-427.2065 757.6153,-431.8227 753.0391,-422.2671 748.079,-427.2065\"/>\n</g>\n<!-- 36&#45;&gt;23 -->\n<g id=\"edge23\" class=\"edge\">\n<title>36&#45;&gt;23</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M700.8908,-333.178C715.0646,-344.4337 733.4508,-359.0345 748.9302,-371.3269\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"747.1779,-374.4047 757.1856,-377.8827 751.5311,-368.9229 747.1779,-374.4047\"/>\n</g>\n<!-- 36&#45;&gt;33 -->\n<g id=\"edge33\" class=\"edge\">\n<title>36&#45;&gt;33</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M709.7143,-323.395C720.2811,-326.1921 732.1433,-329.332 743.0705,-332.2246\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"742.2162,-335.6189 752.7789,-334.7944 744.0075,-328.852 742.2162,-335.6189\"/>\n</g>\n<!-- 36&#45;&gt;35 -->\n<g id=\"edge35\" class=\"edge\">\n<title>36&#45;&gt;35</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M709.7143,-306.605C720.2811,-303.8079 732.1433,-300.668 743.0705,-297.7754\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"744.0075,-301.148 752.7789,-295.2056 742.2162,-294.3811 744.0075,-301.148\"/>\n</g>\n<!-- 37 -->\n<g id=\"node38\" class=\"node\">\n<title>37</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">&#39;&#39;</text>\n</g>\n<!-- 36&#45;&gt;37 -->\n<g id=\"edge37\" class=\"edge\">\n<title>36&#45;&gt;37</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M700.8908,-296.822C715.0646,-285.5663 733.4508,-270.9655 748.9302,-258.6731\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"751.5311,-261.0771 757.1856,-252.1173 747.1779,-255.5953 751.5311,-261.0771\"/>\n</g>\n<!-- 38 -->\n<g id=\"node39\" class=\"node\">\n<title>38</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-176.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">and</text>\n</g>\n<!-- 36&#45;&gt;38 -->\n<g id=\"edge38\" class=\"edge\">\n<title>36&#45;&gt;38</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M687.9364,-296.7455C700.5598,-274.3791 723.7595,-235.8999 749,-207 749.493,-206.4356 749.9976,-205.872 750.5122,-205.3102\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"753.0391,-207.7329 757.6153,-198.1773 748.079,-202.7935 753.0391,-207.7329\"/>\n</g>\n<!-- 39 -->\n<g id=\"node40\" class=\"node\">\n<title>39</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-122.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">``</text>\n</g>\n<!-- 36&#45;&gt;39 -->\n<g id=\"edge39\" class=\"edge\">\n<title>36&#45;&gt;39</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M683.132,-296.6293C692.5501,-265.083 714.8182,-199.523 749,-153 749.3566,-152.5147 749.7233,-152.0321 750.0991,-151.5525\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"752.7206,-153.8718 756.8185,-144.1015 747.5222,-149.1838 752.7206,-153.8718\"/>\n</g>\n<!-- 41 -->\n<g id=\"node42\" class=\"node\">\n<title>41</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">solving</text>\n</g>\n<!-- 36&#45;&gt;41 -->\n<g id=\"edge41\" class=\"edge\">\n<title>36&#45;&gt;41</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M680.6267,-296.79C686.9439,-257.7208 705.7813,-164.122 749,-99 749.1924,-98.7101 749.3887,-98.4215 749.5887,-98.1341\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"752.4823,-100.1306 756.2271,-90.2197 747.119,-95.6322 752.4823,-100.1306\"/>\n</g>\n<!-- 42 -->\n<g id=\"node43\" class=\"node\">\n<title>42</title>\n<text text-anchor=\"middle\" x=\"780\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">&#39;&#39;</text>\n</g>\n<!-- 36&#45;&gt;42 -->\n<g id=\"edge42\" class=\"edge\">\n<title>36&#45;&gt;42</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M679.1157,-296.8388C682.6551,-251.1732 696.7115,-129.2042 749,-45 749.1835,-44.7044 749.3713,-44.4103 749.5632,-44.1177\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"752.4779,-46.0797 756.0473,-36.1042 747.0361,-41.6765 752.4779,-46.0797\"/>\n</g>\n<!-- 40 -->\n<g id=\"node41\" class=\"node\">\n<title>40</title>\n<text text-anchor=\"middle\" x=\"879.5\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">problem</text>\n</g>\n<!-- 41&#45;&gt;40 -->\n<g id=\"edge40\" class=\"edge\">\n<title>41&#45;&gt;40</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M809.5839,-72C818.0746,-72 827.5086,-72 836.6283,-72\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"836.724,-75.5001 846.724,-72 836.724,-68.5001 836.724,-75.5001\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "## 45. Triple with subject, verb, and direct object\n",
        "We are interested in extracting facts from the text. In this chapter, we represent a fact as a tuple of (subject, predicate, object). Extract tuples from dependency trees where:\n",
        "*   subject is a nominal subject of a verb in the past tense\n",
        "*   predicate is the verb in the past tense\n",
        "*   object is a direct object of the verb\n",
        "\n",
        "Consider an example sentence, “Frank Rosenblatt invented the perceptron”. We want to extract a tuple, (Rosenblatt, invented, perceptron), from the sentence. In this problem, we only consider a subject and object as a single word.\n",
        "\n",
        "This graph shows a dependency tree for the sentence (this may vary depending on the parser).\n",
        "\n",
        "<i> check the site for the image </i>\n",
        "\n",
        "In order to extract a tuple from a dependency tree, it may be a good idea to design an extraction rule on the dependency tree, for example,\n",
        "\n",
        "<i> check the site for the image </i>"
      ],
      "metadata": {
        "id": "eaCK-NVzIkTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule: no restriction\n",
        "\n"
      ],
      "metadata": {
        "id": "kH-UkvP2D1oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences):\n",
        "  vbds = []\n",
        "  vbdsId = [] # this Id is needed because some words occured multiple times (distinct each same word in a sentence)\n",
        "\n",
        "  for j, word in enumerate(sentence):\n",
        "    if word._pos == \"VBD\": \n",
        "      vbds.append(word._text)\n",
        "      vbdsId.append(j+1)\n",
        "\n",
        "  for e, vbd in enumerate(vbds):\n",
        "    subj = []\n",
        "    obj = []\n",
        "    \n",
        "    for _, word in enumerate(sentence):\n",
        "      if word._head == vbd and word._headId == vbdsId[e] and word._text != word._head and word._text not in [\",\", \"`\", \";\",\".\",\":\"]:\n",
        "        if word._dep == \"nsubj\": subj.append(word._text)\n",
        "        if word._dep == \"dobj\": obj.append(word._text)\n",
        "\n",
        "    if subj and obj:\n",
        "      print(i, subj, \"----\", vbd, \"----\", obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-bZEHXbDanl",
        "outputId": "bbc6b263-c78e-42c6-ba7c-c9680179012b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23 ['characters'] ---- raised ---- ['many']\n",
            "27 ['this'] ---- led ---- ['researchers']\n",
            "32 ['They'] ---- produced ---- ['programs']\n",
            "37 ['governments'] ---- cut ---- ['research']\n",
            "41 ['project'] ---- inspired ---- ['U.S']\n",
            "48 ['match'] ---- defeated ---- ['champions']\n",
            "49 ['computers'] ---- enabled ---- ['advances']\n",
            "51 ['AlphaGo'] ---- won ---- ['4', 'games']\n",
            "52 ['AlphaGo'] ---- won ---- ['match']\n",
            "52 ['who'] ---- held ---- ['ranking']\n",
            "53 ['This'] ---- marked ---- ['completion']\n",
            "58 ['they'] ---- had ---- ['AI']\n",
            "59 ['China'] ---- accelerated ---- ['funding']\n",
            "88 ['that'] ---- undiscovered ---- ['swans']\n",
            "99 ['they'] ---- advocated ---- ['violence']\n",
            "113 ['researchers'] ---- developed ---- ['algorithms']\n",
            "176 ['DeepMind'] ---- developed ---- ['intelligence']\n",
            "189 ['number'] ---- explored ---- ['connection']\n",
            "190 ['Some'] ---- built ---- ['machines']\n",
            "190 ['that'] ---- used ---- ['networks']\n",
            "194 ['one'] ---- developed ---- ['style']\n",
            "199 ['Simon'] ---- studied ---- ['skills']\n",
            "199 ['work'] ---- laid ---- ['foundations']\n",
            "200 ['team'] ---- used ---- ['results']\n",
            "202 ['people'] ---- used ---- ['algorithms']\n",
            "206 ['Schank'] ---- described ---- ['approaches']\n",
            "209 ['revolution'] ---- led ---- ['form']\n",
            "216 ['Researchers'] ---- rejected ---- ['AI']\n",
            "225 ['researchers'] ---- adopted ---- ['tools']\n",
            "226 ['language'] ---- permitted ---- ['level']\n",
            "294 ['each'] ---- cast ---- ['vote']\n",
            "302 ['Rosenblatt'] ---- invented ---- ['perceptron']\n",
            "316 ['Aizenberg'] ---- introduced ---- ['it']\n",
            "320 ['publication'] ---- introduced ---- ['way']\n",
            "324 ['LeCun'] ---- applied ---- ['backpropagation']\n",
            "327 ['that'] ---- beat ---- ['champion']\n",
            "332 ['NN'] ---- called ---- ['network']\n",
            "335 ['recognition'] ---- experienced ---- ['jump']\n",
            "336 ['Google'] ---- used ---- ['LSTM']\n",
            "344 ['AlphaGo'] ---- brought ---- ['era']\n",
            "369 ['formula'] ---- determined ---- ['dose']\n",
            "380 ['machine'] ---- performed ---- ['diagnosis']\n",
            "381 ['study'] ---- demonstrated ---- ['surgery']\n",
            "422 ['AICPA'] ---- introduced ---- ['course']\n",
            "431 ['AI'] ---- managed ---- ['systems']\n",
            "459 ['which'] ---- took ---- ['place']\n",
            "460 ['Association'] ---- dedicated ---- ['issue']\n",
            "461 ['Electronica', 'Vienna'] ---- opened ---- ['exhibitions']\n",
            "467 ['Scientists'] ---- described ---- ['goals', 'laws']\n",
            "489 ['Musk'] ---- donated ---- ['$']\n",
            "538 ['he'] ---- named ---- ['problems', 'which', 'problems']\n",
            "561 ['research'] ---- produced ---- ['software']\n",
            "572 ['survey'] ---- showed ---- ['disagreement']\n",
            "576 ['Union'] ---- published ---- ['paper']\n",
            "581 ['Asimov'] ---- introduced ---- ['Laws', 'series']\n",
            "585 ['Sorayama'] ---- considered ---- ['robots']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule: subject must be NNP or NN\n"
      ],
      "metadata": {
        "id": "coM1YKpsENv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences):\n",
        "  vbds = []\n",
        "  vbdsId = []\n",
        "\n",
        "  for j, word in enumerate(sentence):\n",
        "    if word._pos == \"VBD\": \n",
        "      vbds.append(word._text)\n",
        "      vbdsId.append(j+1)\n",
        "\n",
        "  for e, vbd in enumerate(vbds):\n",
        "    subj = []\n",
        "    obj = []\n",
        "    \n",
        "    for _, word in enumerate(sentence):\n",
        "      if word._head == vbd and word._headId == vbdsId[e] and word._text != word._head and word._text not in [\",\", \"`\", \";\",\".\",\":\"]:\n",
        "        if word._dep == \"nsubj\" and word._pos in [\"NNP\",\"NN\"]: subj.append(word._text)\n",
        "        if word._dep == \"dobj\": obj.append(word._text)\n",
        "\n",
        "    if subj and obj:\n",
        "      print(i, subj, \"----\", vbd, \"----\", obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1_fmLVgEQxJ",
        "outputId": "76c72b14-8f8b-4215-ea8f-dae4179a5328"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41 ['project'] ---- inspired ---- ['U.S']\n",
            "48 ['match'] ---- defeated ---- ['champions']\n",
            "51 ['AlphaGo'] ---- won ---- ['4', 'games']\n",
            "52 ['AlphaGo'] ---- won ---- ['match']\n",
            "59 ['China'] ---- accelerated ---- ['funding']\n",
            "176 ['DeepMind'] ---- developed ---- ['intelligence']\n",
            "189 ['number'] ---- explored ---- ['connection']\n",
            "199 ['Simon'] ---- studied ---- ['skills']\n",
            "199 ['work'] ---- laid ---- ['foundations']\n",
            "200 ['team'] ---- used ---- ['results']\n",
            "206 ['Schank'] ---- described ---- ['approaches']\n",
            "209 ['revolution'] ---- led ---- ['form']\n",
            "226 ['language'] ---- permitted ---- ['level']\n",
            "302 ['Rosenblatt'] ---- invented ---- ['perceptron']\n",
            "316 ['Aizenberg'] ---- introduced ---- ['it']\n",
            "320 ['publication'] ---- introduced ---- ['way']\n",
            "324 ['LeCun'] ---- applied ---- ['backpropagation']\n",
            "332 ['NN'] ---- called ---- ['network']\n",
            "335 ['recognition'] ---- experienced ---- ['jump']\n",
            "336 ['Google'] ---- used ---- ['LSTM']\n",
            "344 ['AlphaGo'] ---- brought ---- ['era']\n",
            "369 ['formula'] ---- determined ---- ['dose']\n",
            "380 ['machine'] ---- performed ---- ['diagnosis']\n",
            "381 ['study'] ---- demonstrated ---- ['surgery']\n",
            "422 ['AICPA'] ---- introduced ---- ['course']\n",
            "431 ['AI'] ---- managed ---- ['systems']\n",
            "460 ['Association'] ---- dedicated ---- ['issue']\n",
            "461 ['Electronica', 'Vienna'] ---- opened ---- ['exhibitions']\n",
            "489 ['Musk'] ---- donated ---- ['$']\n",
            "561 ['research'] ---- produced ---- ['software']\n",
            "572 ['survey'] ---- showed ---- ['disagreement']\n",
            "576 ['Union'] ---- published ---- ['paper']\n",
            "581 ['Asimov'] ---- introduced ---- ['Laws', 'series']\n",
            "585 ['Sorayama'] ---- considered ---- ['robots']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule: subject must be NNP\n",
        "\n"
      ],
      "metadata": {
        "id": "1-K_sMEoD8yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences):\n",
        "  vbds = []\n",
        "  vbdsId = []\n",
        "\n",
        "  for j, word in enumerate(sentence):\n",
        "    if word._pos == \"VBD\": \n",
        "      vbds.append(word._text)\n",
        "      vbdsId.append(j+1)\n",
        "\n",
        "  for e, vbd in enumerate(vbds):\n",
        "    subj = []\n",
        "    obj = []\n",
        "    \n",
        "    for _, word in enumerate(sentence):\n",
        "      if word._head == vbd and word._headId == vbdsId[e] and word._text != word._head and word._text not in [\",\", \"`\", \";\",\".\",\":\"]:\n",
        "        if word._dep == \"nsubj\" and word._pos == \"NNP\": subj.append(word._text)\n",
        "        if word._dep == \"dobj\": obj.append(word._text)\n",
        "  \n",
        "    if subj and obj:\n",
        "      print(i, subj, \"----\", vbd, \"----\", obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyMwSyZt_L3O",
        "outputId": "d0e6aee5-087f-4b6e-91bd-bcda61c74550"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51 ['AlphaGo'] ---- won ---- ['4', 'games']\n",
            "52 ['AlphaGo'] ---- won ---- ['match']\n",
            "59 ['China'] ---- accelerated ---- ['funding']\n",
            "176 ['DeepMind'] ---- developed ---- ['intelligence']\n",
            "199 ['Simon'] ---- studied ---- ['skills']\n",
            "206 ['Schank'] ---- described ---- ['approaches']\n",
            "302 ['Rosenblatt'] ---- invented ---- ['perceptron']\n",
            "316 ['Aizenberg'] ---- introduced ---- ['it']\n",
            "324 ['LeCun'] ---- applied ---- ['backpropagation']\n",
            "332 ['NN'] ---- called ---- ['network']\n",
            "336 ['Google'] ---- used ---- ['LSTM']\n",
            "422 ['AICPA'] ---- introduced ---- ['course']\n",
            "431 ['AI'] ---- managed ---- ['systems']\n",
            "460 ['Association'] ---- dedicated ---- ['issue']\n",
            "461 ['Electronica', 'Vienna'] ---- opened ---- ['exhibitions']\n",
            "489 ['Musk'] ---- donated ---- ['$']\n",
            "576 ['Union'] ---- published ---- ['paper']\n",
            "581 ['Asimov'] ---- introduced ---- ['Laws', 'series']\n",
            "585 ['Sorayama'] ---- considered ---- ['robots']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "## 46. Expanding subjects and objects\n",
        "Improve the program of Problem 45 to remove the restriction that subjects and objects are single words but can also be phrases. For example, we want to extract (Frank Rosenblatt, invented, perceptron) from the sentence, “Frank Rosenblatt invented the perceptron”."
      ],
      "metadata": {
        "id": "MLNeL_a4I8Sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule: no restriction"
      ],
      "metadata": {
        "id": "JLOhR6mSsgpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences):\n",
        "  vbds = []\n",
        "  vbdsId = []\n",
        "\n",
        "  for j, word in enumerate(sentence):\n",
        "    if word._pos == \"VBD\": \n",
        "      vbds.append(word._text)\n",
        "      vbdsId.append(j+1)\n",
        "\n",
        "  for e, vbd in enumerate(vbds):\n",
        "    subj = []\n",
        "    obj = []\n",
        "    \n",
        "    for j, word in enumerate(sentence):\n",
        "      if word._head == vbd and word._headId == vbdsId[e] and word._text != word._head and word._text not in [\",\", \"`\", \";\",\".\",\":\"] :\n",
        "        if word._dep in [\"nsubj\", \"dobj\"]:\n",
        "          phrase = []\n",
        "          phrase.append(word._text)\n",
        "          prevIndex = j - 1\n",
        "          nextIndex = j + 1\n",
        "          \n",
        "          #lookback sub\n",
        "          currentWord = word._text\n",
        "          while prevIndex > 0 and sentence[prevIndex]._head == currentWord and sentence[prevIndex]._dep == \"compound\":\n",
        "            phrase.insert(0, sentence[prevIndex]._text)\n",
        "            currentWord = sentence[prevIndex]._text\n",
        "            prevIndex -= prevIndex\n",
        "\n",
        "          #lookfront sub\n",
        "          currentWord = word._text\n",
        "          if nextIndex < len(sentence):\n",
        "            while sentence[nextIndex]._head == currentWord and sentence[nextIndex]._dep == \"compound\":\n",
        "              phrase.append(sentence[nextIndex]._text)\n",
        "              currentWord = sentence[nextIndex]._text\n",
        "              nextIndex += nextIndex\n",
        "\n",
        "        if word._dep == \"nsubj\": \n",
        "          subj.append(\" \".join(phrase))\n",
        "        elif word._dep == \"dobj\":\n",
        "          obj.append(\" \".join(phrase))\n",
        "\n",
        "    if subj and obj:\n",
        "      print(i, subj, \"----\", vbd, \"----\", obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm3E37nwEQGD",
        "outputId": "bec31903-29cf-4058-9f37-c2ef2e5e355a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23 ['characters'] ---- raised ---- ['many']\n",
            "27 ['this'] ---- led ---- ['researchers']\n",
            "32 ['They'] ---- produced ---- ['programs']\n",
            "37 ['governments'] ---- cut ---- ['research']\n",
            "41 ['computer project'] ---- inspired ---- ['U.S']\n",
            "48 ['exhibition match'] ---- defeated ---- ['champions']\n",
            "49 ['computers'] ---- enabled ---- ['advances']\n",
            "51 ['AlphaGo'] ---- won ---- ['4', 'games']\n",
            "52 ['AlphaGo'] ---- won ---- ['match']\n",
            "52 ['who'] ---- held ---- ['ranking']\n",
            "53 ['This'] ---- marked ---- ['completion']\n",
            "58 ['they'] ---- had ---- ['AI']\n",
            "59 ['China'] ---- accelerated ---- ['government funding']\n",
            "88 ['that'] ---- undiscovered ---- ['swans']\n",
            "99 ['they'] ---- advocated ---- ['violence']\n",
            "113 ['researchers'] ---- developed ---- ['algorithms']\n",
            "176 ['DeepMind'] ---- developed ---- ['intelligence']\n",
            "189 ['number'] ---- explored ---- ['connection']\n",
            "190 ['Some'] ---- built ---- ['machines']\n",
            "190 ['that'] ---- used ---- ['networks']\n",
            "194 ['one'] ---- developed ---- ['style']\n",
            "199 ['Herbert Simon'] ---- studied ---- ['skills']\n",
            "199 ['work'] ---- laid ---- ['foundations']\n",
            "200 ['research team'] ---- used ---- ['results']\n",
            "202 ['people'] ---- used ---- ['algorithms']\n",
            "206 ['Schank'] ---- described ---- ['approaches']\n",
            "209 ['knowledge revolution'] ---- led ---- ['form']\n",
            "216 ['Researchers'] ---- rejected ---- ['AI']\n",
            "225 ['AI researchers'] ---- adopted ---- ['tools']\n",
            "226 ['language'] ---- permitted ---- ['level']\n",
            "294 ['each'] ---- cast ---- ['vote']\n",
            "302 ['Rosenblatt'] ---- invented ---- ['perceptron']\n",
            "316 ['Igor Aizenberg'] ---- introduced ---- ['it']\n",
            "320 ['publication'] ---- introduced ---- ['way']\n",
            "324 ['Yann LeCun'] ---- applied ---- ['backpropagation']\n",
            "327 ['that'] ---- beat ---- ['Go champion']\n",
            "332 ['NN'] ---- called ---- ['network']\n",
            "335 ['speech recognition'] ---- experienced ---- ['performance jump']\n",
            "336 ['Google'] ---- used ---- ['LSTM']\n",
            "344 ['AlphaGo'] ---- brought ---- ['era']\n",
            "369 ['formula'] ---- determined ---- ['dose']\n",
            "380 ['machine'] ---- performed ---- ['diagnosis']\n",
            "381 ['study'] ---- demonstrated ---- ['surgery']\n",
            "422 ['AICPA'] ---- introduced ---- ['training course']\n",
            "431 ['AI'] ---- managed ---- ['signal systems']\n",
            "459 ['which'] ---- took ---- ['place']\n",
            "460 ['Association'] ---- dedicated ---- ['magazine issue']\n",
            "461 ['Ars Electronica', 'Vienna'] ---- opened ---- ['exhibitions']\n",
            "467 ['Scientists'] ---- described ---- ['research goals', 'laws']\n",
            "489 ['Musk'] ---- donated ---- ['$']\n",
            "538 ['he'] ---- named ---- ['problems', 'which', 'problems']\n",
            "561 ['research'] ---- produced ---- ['software']\n",
            "572 ['survey'] ---- showed ---- ['disagreement']\n",
            "576 ['European Union'] ---- published ---- ['strategy paper']\n",
            "581 ['Asimov'] ---- introduced ---- ['Laws', 'series']\n",
            "585 ['Sorayama'] ---- considered ---- ['robots']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule: subject must be NN or NNP"
      ],
      "metadata": {
        "id": "lGA8KDn2Mz82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences):\n",
        "  vbds = []\n",
        "  vbdsId = []\n",
        "\n",
        "  for j, word in enumerate(sentence):\n",
        "    if word._pos == \"VBD\": \n",
        "      vbds.append(word._text)\n",
        "      vbdsId.append(j+1)\n",
        "\n",
        "  for e, vbd in enumerate(vbds):\n",
        "    subj = []\n",
        "    obj = []\n",
        "    \n",
        "    for j, word in enumerate(sentence):\n",
        "      if word._head == vbd and word._headId == vbdsId[e] and word._text != word._head and word._text not in [\",\", \"`\", \";\",\".\",\":\"]:\n",
        "        if word._dep == \"nsubj\" and word._pos in [\"NNP\", \"NN\"]: \n",
        "          phrase = []\n",
        "          phrase.append(word._text)\n",
        "          prevIndex = j - 1\n",
        "          nextIndex = j + 1\n",
        "          \n",
        "          #lookback sub\n",
        "          currentWord = word._text\n",
        "          while prevIndex > 0 and sentence[prevIndex]._head == currentWord and sentence[prevIndex]._dep == \"compound\":\n",
        "            phrase.insert(0, sentence[prevIndex]._text)\n",
        "            currentWord = sentence[prevIndex]._text\n",
        "            prevIndex -= prevIndex\n",
        "\n",
        "          #lookfront sub\n",
        "          currentWord = word._text\n",
        "          if nextIndex < len(sentence):\n",
        "            while sentence[nextIndex]._head == currentWord and sentence[nextIndex]._dep == \"compound\":\n",
        "              phrase.append(sentence[nextIndex]._text)\n",
        "              currentWord = sentence[nextIndex]._text\n",
        "              nextIndex += nextIndex\n",
        "\n",
        "          subj.append(\" \".join(phrase))\n",
        "\n",
        "        if word._dep == \"dobj\":\n",
        "          phrase = []\n",
        "          phrase.append(word._text)\n",
        "          prevIndex = j - 1\n",
        "          nextIndex = j + 1\n",
        "          \n",
        "          #lookback sub\n",
        "          currentWord = word._text\n",
        "          while prevIndex > 0 and sentence[prevIndex]._head == currentWord and sentence[prevIndex]._dep == \"compound\":\n",
        "            phrase.insert(0, sentence[prevIndex]._text)\n",
        "            currentWord = sentence[prevIndex]._text\n",
        "            prevIndex -= prevIndex\n",
        "\n",
        "          #lookfront sub\n",
        "          currentWord = word._text\n",
        "          if nextIndex < len(sentence):\n",
        "            while sentence[nextIndex]._head == currentWord and sentence[nextIndex]._dep == \"compound\":\n",
        "              phrase.append(sentence[nextIndex]._text)\n",
        "              currentWord = sentence[nextIndex]._text\n",
        "              nextIndex += nextIndex\n",
        "\n",
        "          obj.append(\" \".join(phrase))\n",
        "\n",
        "    if subj and obj:\n",
        "      print(i, subj, \"----\", vbd, \"----\", obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIDzRnb7M2MX",
        "outputId": "d2039ba6-b14d-42ed-a790-b48321e1f4d6"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41 ['computer project'] ---- inspired ---- ['U.S']\n",
            "48 ['exhibition match'] ---- defeated ---- ['champions']\n",
            "51 ['AlphaGo'] ---- won ---- ['4', 'games']\n",
            "52 ['AlphaGo'] ---- won ---- ['match']\n",
            "59 ['China'] ---- accelerated ---- ['government funding']\n",
            "176 ['DeepMind'] ---- developed ---- ['intelligence']\n",
            "189 ['number'] ---- explored ---- ['connection']\n",
            "199 ['Herbert Simon'] ---- studied ---- ['skills']\n",
            "199 ['work'] ---- laid ---- ['foundations']\n",
            "200 ['research team'] ---- used ---- ['results']\n",
            "206 ['Schank'] ---- described ---- ['approaches']\n",
            "209 ['knowledge revolution'] ---- led ---- ['form']\n",
            "226 ['language'] ---- permitted ---- ['level']\n",
            "302 ['Rosenblatt'] ---- invented ---- ['perceptron']\n",
            "316 ['Igor Aizenberg'] ---- introduced ---- ['it']\n",
            "320 ['publication'] ---- introduced ---- ['way']\n",
            "324 ['Yann LeCun'] ---- applied ---- ['backpropagation']\n",
            "332 ['NN'] ---- called ---- ['network']\n",
            "335 ['speech recognition'] ---- experienced ---- ['performance jump']\n",
            "336 ['Google'] ---- used ---- ['LSTM']\n",
            "344 ['AlphaGo'] ---- brought ---- ['era']\n",
            "369 ['formula'] ---- determined ---- ['dose']\n",
            "380 ['machine'] ---- performed ---- ['diagnosis']\n",
            "381 ['study'] ---- demonstrated ---- ['surgery']\n",
            "422 ['AICPA'] ---- introduced ---- ['training course']\n",
            "431 ['AI'] ---- managed ---- ['signal systems']\n",
            "460 ['Association'] ---- dedicated ---- ['magazine issue']\n",
            "461 ['Ars Electronica', 'Vienna'] ---- opened ---- ['exhibitions']\n",
            "489 ['Musk'] ---- donated ---- ['$']\n",
            "561 ['research'] ---- produced ---- ['software']\n",
            "572 ['survey'] ---- showed ---- ['disagreement']\n",
            "576 ['European Union'] ---- published ---- ['strategy paper']\n",
            "581 ['Asimov'] ---- introduced ---- ['Laws', 'series']\n",
            "585 ['Sorayama'] ---- considered ---- ['robots']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rule: subject must be NNP\n"
      ],
      "metadata": {
        "id": "ATuVvA6HE13d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences):\n",
        "  vbds = []\n",
        "  vbdsId = []\n",
        "\n",
        "  for j, word in enumerate(sentence):\n",
        "    if word._pos == \"VBD\": \n",
        "      vbds.append(word._text)\n",
        "      vbdsId.append(j+1)\n",
        "\n",
        "  for e, vbd in enumerate(vbds):\n",
        "    subj = []\n",
        "    obj = []\n",
        "    \n",
        "    for j, word in enumerate(sentence):\n",
        "      if word._head == vbd and word._headId == vbdsId[e] and word._text != word._head and word._text not in [\",\", \"`\", \";\",\".\",\":\"]:\n",
        "        if word._dep == \"nsubj\" and word._pos == \"NNP\": \n",
        "          phrase = []\n",
        "          phrase.append(word._text)\n",
        "          prevIndex = j - 1\n",
        "          nextIndex = j + 1\n",
        "          \n",
        "          #lookback sub\n",
        "          currentWord = word._text\n",
        "          while prevIndex > 0 and sentence[prevIndex]._head == currentWord and sentence[prevIndex]._dep == \"compound\":\n",
        "            phrase.insert(0, sentence[prevIndex]._text)\n",
        "            currentWord = sentence[prevIndex]._text\n",
        "            prevIndex -= prevIndex\n",
        "\n",
        "          #lookfront sub\n",
        "          currentWord = word._text\n",
        "          if nextIndex < len(sentence):\n",
        "            while sentence[nextIndex]._head == currentWord and sentence[nextIndex]._dep == \"compound\":\n",
        "              phrase.append(sentence[nextIndex]._text)\n",
        "              currentWord = sentence[nextIndex]._text\n",
        "              nextIndex += nextIndex\n",
        "\n",
        "          subj.append(\" \".join(phrase))\n",
        "\n",
        "        if word._dep == \"dobj\":\n",
        "          phrase = []\n",
        "          phrase.append(word._text)\n",
        "          prevIndex = j - 1\n",
        "          nextIndex = j + 1\n",
        "          \n",
        "          #lookback sub\n",
        "          currentWord = word._text\n",
        "          while prevIndex > 0 and sentence[prevIndex]._head == currentWord and sentence[prevIndex]._dep == \"compound\":\n",
        "            phrase.insert(0, sentence[prevIndex]._text)\n",
        "            currentWord = sentence[prevIndex]._text\n",
        "            prevIndex -= prevIndex\n",
        "\n",
        "          #lookfront sub\n",
        "          currentWord = word._text\n",
        "          if nextIndex < len(sentence):\n",
        "            while sentence[nextIndex]._head == currentWord and sentence[nextIndex]._dep == \"compound\":\n",
        "              phrase.append(sentence[nextIndex]._text)\n",
        "              currentWord = sentence[nextIndex]._text\n",
        "              nextIndex += nextIndex\n",
        "\n",
        "          obj.append(\" \".join(phrase))\n",
        "\n",
        "    if subj and obj:\n",
        "      print(i, subj, \"----\", vbd, \"----\", obj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2XUJ_SsGKlG",
        "outputId": "84f397d4-1bde-4341-bce5-cbd68335ebe2"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51 ['AlphaGo'] ---- won ---- ['4', 'games']\n",
            "52 ['AlphaGo'] ---- won ---- ['match']\n",
            "59 ['China'] ---- accelerated ---- ['government funding']\n",
            "176 ['DeepMind'] ---- developed ---- ['intelligence']\n",
            "199 ['Herbert Simon'] ---- studied ---- ['skills']\n",
            "206 ['Schank'] ---- described ---- ['approaches']\n",
            "302 ['Rosenblatt'] ---- invented ---- ['perceptron']\n",
            "316 ['Igor Aizenberg'] ---- introduced ---- ['it']\n",
            "324 ['Yann LeCun'] ---- applied ---- ['backpropagation']\n",
            "332 ['NN'] ---- called ---- ['network']\n",
            "336 ['Google'] ---- used ---- ['LSTM']\n",
            "422 ['AICPA'] ---- introduced ---- ['training course']\n",
            "431 ['AI'] ---- managed ---- ['signal systems']\n",
            "460 ['Association'] ---- dedicated ---- ['magazine issue']\n",
            "461 ['Ars Electronica', 'Vienna'] ---- opened ---- ['exhibitions']\n",
            "489 ['Musk'] ---- donated ---- ['$']\n",
            "576 ['European Union'] ---- published ---- ['strategy paper']\n",
            "581 ['Asimov'] ---- introduced ---- ['Laws', 'series']\n",
            "585 ['Sorayama'] ---- considered ---- ['robots']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "## 47. Triple from the passive sentence\n",
        "Extract facts from sentences in the passive voice. Consider an example sentence, “Artificial intelligence was founded as an academic discipline in 1955”. We want to extract two tuples from the sentence,\n",
        "\n",
        "* (Artificial intelligence, founded-as, academic discipline)\n",
        "* (Artificial intelligence, founded-in, 1955)"
      ],
      "metadata": {
        "id": "jWtjRYCAJC5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences):\n",
        "  vbds = []\n",
        "  vbdsId = []\n",
        "\n",
        "  for j, word in enumerate(sentence):\n",
        "    if j is not 0:\n",
        "      if word._pos == \"VBN\" and sentence[j-1]._dep == \"auxpass\":\n",
        "        vbds.append(word._text)\n",
        "        vbdsId.append(j+1)\n",
        "\n",
        "  for e, vbd in enumerate(vbds):\n",
        "    passiveSubj = []\n",
        "    fact = []\n",
        "    \n",
        "    for _, word in enumerate(sentence):\n",
        "      if word._head == vbd and word._headId == vbdsId[e] and word._text != word._head:\n",
        "        if word._dep in [\"nsubj\", \"nsubjpass\"]  and word._text not in [\",\", \"`\", \";\",\".\",\":\"]: passiveSubj.append(word._text)\n",
        "        if word._dep in [\"nmod\"] and word._id > vbdsId[e]:\n",
        "          fact.append(word._text)\n",
        "\n",
        "    if passiveSubj and fact:\n",
        "      print(i, passiveSubj, \"----\", vbd, \"----\", fact)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqBRJYxJrHp9",
        "outputId": "80577292-2ef2-4938-e085-40c961fb8afd"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 ['intelligence'] ---- founded ---- ['discipline', '1955']\n",
            "8 ['research'] ---- divided ---- ['sub-fields']\n",
            "9 ['sub-fields'] ---- based ---- ['considerations']\n",
            "10 ['Sub-fields'] ---- based ---- ['factors']\n",
            "14 ['tools'] ---- used ---- ['AI', 'versions']\n",
            "16 ['field'] ---- founded ---- ['assumption']\n",
            "18 ['issues'] ---- explored ---- ['myth', 'antiquity']\n",
            "26 ['insight'] ---- known ---- ['Church']\n",
            "30 ['term'] ---- coined ---- ['McCarthy']\n",
            "39 ['research'] ---- revived ---- ['success']\n",
            "66 ['AI'] ---- programmed ---- ['learning']\n",
            "91 ['data'] ---- known ---- ['overfitting']\n",
            "110 ['problem'] ---- broken ---- ['sub-problems']\n",
            "123 ['semantics'] ---- captured ---- ['concepts']\n",
            "125 ['representations'] ---- used ---- ['indexing']\n",
            "133 ['behavior'] ---- used ---- ['algorithms']\n",
            "139 ['classifiers'] ---- viewed ---- ['approximators', 'example']\n",
            "162 ['This'] ---- attributed ---- ['fact']\n",
            "163 ['paradox'] ---- extended ---- ['forms']\n",
            "174 ['work'] ---- incorporated ---- ['machine']\n",
            "181 ['what'] ---- talked ---- ['about']\n",
            "192 ['elements'] ---- revived ---- ['1980s']\n",
            "193 ['intelligence'] ---- reduced ---- ['manipulation']\n",
            "207 ['they'] ---- built ---- ['hand']\n",
            "211 ['amounts'] ---- required ---- ['applications']\n",
            "218 ['aspects'] ---- required ---- ['intelligence']\n",
            "220 ['Interest'] ---- revived ---- ['Rumelhart', 'middle']\n",
            "221 ['which'] ---- solved ---- ['certainty']\n",
            "223 ['application'] ---- studied ---- ['discipline']\n",
            "224 ['Much'] ---- bogged ---- ['patches']\n",
            "235 ['problems'] ---- solved ---- ['theory']\n",
            "248 ['algorithms'] ---- visualized ---- ['climbing']\n",
            "255 ['Logic'] ---- used ---- ['representation']\n",
            "255 ['it'] ---- applied ---- ['problems']\n",
            "257 ['forms'] ---- used ---- ['research']\n",
            "265 ['Logics'] ---- designed ---- ['logics']\n",
            "268 ['that'] ---- used ---- ['problems']\n",
            "269 ['algorithms'] ---- used ---- ['filtering']\n",
            "278 ['applications'] ---- divided ---- ['types']\n",
            "281 ['They'] ---- tuned ---- ['examples']\n",
            "282 ['examples'] ---- known ---- ['observations']\n",
            "284 ['class'] ---- seen ---- ['decision']\n",
            "285 ['observations'] ---- known ---- ['set']\n",
            "287 ['classifier'] ---- trained ---- ['ways']\n",
            "293 ['networks'] ---- inspired ---- ['architecture']\n",
            "306 ['networks'] ---- applied ---- ['problem']\n",
            "316 ['expression'] ---- introduced ---- ['machine']\n",
            "317 ['networks'] ---- published ---- ['Ivakhnenko', '1965']\n",
            "318 ['networks'] ---- trained ---- ['time']\n",
            "323 ['origins'] ---- traced ---- ['Neocognitron']\n",
            "327 ['CNNs'] ---- used ---- ['conjunction']\n",
            "330 ['RNNs'] ---- trained ---- ['descent']\n",
            "360 ['phenomenon'] ---- described ---- ['effect']\n",
            "377 ['that'] ---- worked ---- ['moment']\n",
            "380 ['study'] ---- done ---- ['learning']\n",
            "380 ['patient'] ---- referred ---- ['treatment']\n",
            "390 ['systems'] ---- integrated ---- ['vehicle']\n",
            "394 ['that'] ---- used ---- ['highway']\n",
            "414 ['AI'] ---- used ---- ['corporations']\n",
            "428 ['intelligence'] ---- used ---- ['surveillance']\n",
            "435 ['industry'] ---- focused ---- ['learning']\n",
            "445 ['AI'] ---- incorporated ---- ['operations']\n",
            "450 ['services'] ---- represented ---- ['form']\n",
            "518 ['field'] ---- delineated ---- ['Symposium', 'Ethics']\n",
            "522 ['ethics', 'ethics'] ---- concerned ---- ['behavior']\n",
            "526 ['variety'] ---- found ---- ['edition']\n",
            "550 ['position'] ---- inspired ---- ['work']\n",
            "583 ['Transhumanism'] ---- explored ---- ['Ghost']\n",
            "584 ['series'] ---- painted ---- ['Japan']\n",
            "584 ['that'] ---- used ---- ['makers']\n",
            "589 ['understanding'] ---- altered ---- ['technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "## 48. Extract paths from the root to nouns\n",
        "For every noun in a dependency tree, extract a path from the root to the noun. Here, each path must satisfy the following specifications.\n",
        "\n",
        "* Nodes in a path are words in surface form\n",
        "* Nodes are connected with “ -> “ from the root to the leaf node\n",
        "* We don’t have to include dependency types (e.g., nsubj, dobj) when representing a dependency path.\n",
        "\n",
        "For the example sentence, “Frank Rosenblatt invented the perceptron”, we expect an output,\n",
        "\n",
        "<i> check the site for the image </i>"
      ],
      "metadata": {
        "id": "L7XEmHNTJULh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences[1:3]):\n",
        "  nouns = []\n",
        "  sentenceCopy = sentence.copy()\n",
        "\n",
        "  for _, word in enumerate(sentenceCopy):\n",
        "    if word._pos in [\"NN\", \"NNS\", \"NNP\"]:\n",
        "      nouns.append(word)\n",
        "\n",
        "  print(\"\\n=== sentence\",i+1,\"nouns:\", [noun._text for noun in nouns],\"===\\n\")\n",
        "\n",
        "  for _, noun in enumerate(nouns):\n",
        "    path = []\n",
        "    currentNoun = noun._text\n",
        "    currentNounId = noun._id\n",
        "    currentHead = noun._head\n",
        "    currentHeadId = noun._headId\n",
        "\n",
        "    path.append(currentNoun + \" (\" + str(currentNounId) + \")\")\n",
        "\n",
        "    if currentHead != \"ROOT\":\n",
        "      while \"ROOT\" != currentHead:\n",
        "        for k, word in enumerate(sentenceCopy):\n",
        "          if word._text == currentHead and word._id == currentHeadId and currentHeadId != currentNounId:\n",
        "            path.insert(0, currentHead + \" (\" + str(currentHeadId) +\")\")\n",
        "            currentNoun = word._text\n",
        "            currentNounId = word._id\n",
        "            currentHead = word._head\n",
        "            currentHeadId = word._headId\n",
        "\n",
        "    path.insert(0, \"ROOT\")\n",
        "    print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MlJzQSby80j",
        "outputId": "db6dc271-d0bd-4774-a6e6-8b3755779ea6"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== sentence 1 nouns: ['AI', 'textbooks', 'field', 'study', 'agents', 'device', 'environment', 'actions', 'chance', 'goals'] ===\n",
            "\n",
            "['ROOT', 'define (4)', 'textbooks (3)', 'AI (2)']\n",
            "['ROOT', 'define (4)', 'textbooks (3)']\n",
            "['ROOT', 'define (4)', 'field (6)']\n",
            "['ROOT', 'define (4)', 'study (9)']\n",
            "['ROOT', 'define (4)', 'study (9)', 'agents (13)']\n",
            "['ROOT', 'define (4)', 'study (9)', 'agents (13)', 'device (17)']\n",
            "['ROOT', 'define (4)', 'study (9)', 'agents (13)', 'device (17)', 'perceives (19)', 'environment (21)']\n",
            "['ROOT', 'define (4)', 'study (9)', 'agents (13)', 'device (17)', 'perceives (19)', 'takes (23)', 'actions (24)']\n",
            "['ROOT', 'define (4)', 'study (9)', 'agents (13)', 'device (17)', 'perceives (19)', 'takes (23)', 'actions (24)', 'maximize (26)', 'chance (28)']\n",
            "['ROOT', 'define (4)', 'study (9)', 'agents (13)', 'device (17)', 'perceives (19)', 'takes (23)', 'actions (24)', 'maximize (26)', 'chance (28)', 'achieving (31)', 'goals (33)']\n",
            "\n",
            "=== sentence 2 nouns: ['term', 'intelligence', 'machines', 'computers', 'functions', 'humans', 'mind', 'problem'] ===\n",
            "\n",
            "['ROOT', 'used (11)', 'intelligence (7)', 'term (4)']\n",
            "['ROOT', 'used (11)', 'intelligence (7)']\n",
            "['ROOT', 'used (11)', 'describe (13)', 'machines (14)']\n",
            "['ROOT', 'used (11)', 'describe (13)', 'machines (14)', 'computers (17)']\n",
            "['ROOT', 'used (11)', 'describe (13)', 'machines (14)', 'mimic (20)', 'functions (24)']\n",
            "['ROOT', 'used (11)', 'describe (13)', 'machines (14)', 'mimic (20)', 'functions (24)', 'associate (27)', 'humans (26)']\n",
            "['ROOT', 'used (11)', 'describe (13)', 'machines (14)', 'mimic (20)', 'functions (24)', 'associate (27)', 'mind (31)']\n",
            "['ROOT', 'used (11)', 'describe (13)', 'machines (14)', 'mimic (20)', 'functions (24)', 'learning (36)', 'solving (41)', 'problem (40)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "## 49. Extract the shortest path between two nouns\n",
        "Extract the shortest path for every pair of two nouns. Supposing that two nouns appear at the i-th and j-th positions (in words) in a sentence (i<j), the shortest path must satisfy the following specifications.\n",
        "\n",
        "* Nodes in a path are words in surface form\n",
        "* Nodes corresponding to the i-th and j-th words are replaced with X and Y, respectively.\n",
        "* Nodes are connected with either “ -> “ or “ <- “ from X to Y to represent a direction of a dependency.\n",
        "\n",
        "We can consider two types of dependency paths.\n",
        "\n",
        "* When the j-th word appears on the path from the i-th word to the root: the path from the i-th word to the j-th word\n",
        "* When the i-th and j-th words have the common ancestor (the k-th word) in the dependency tree: the path from the i-th word to the k-th word connected with “ <- “, followed by the path from the k-th word to the j-th word connected with “ -> “.\n",
        "\n",
        "For the example sentence, “Frank Rosenblatt invented the perceptron”, we expect an output,\n",
        "\n",
        "<i> check the site for the image </i>"
      ],
      "metadata": {
        "id": "IHREHv_mJfN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(ai._sentences[1:2]):\n",
        "  nouns = []\n",
        "  sentenceCopy = sentence.copy()\n",
        "  paths = []\n",
        "\n",
        "  for _, word in enumerate(sentenceCopy):\n",
        "    if word._pos in [\"NN\", \"NNS\", \"NNP\"]:\n",
        "      nouns.append(word)\n",
        "\n",
        "  print(\"\\n=== sentence\",i+1,\"nouns:\", [noun._text for noun in nouns],\"===\\n\")\n",
        "\n",
        "  for _, noun in enumerate(nouns):\n",
        "    path = []\n",
        "    currentNoun = noun._text\n",
        "    currentNounId = noun._id\n",
        "    currentHead = noun._head\n",
        "    currentHeadId = noun._headId\n",
        "\n",
        "    path.append(currentNoun + \" (\" + str(currentNounId) + \")\")\n",
        "\n",
        "    if currentHead != \"ROOT\":\n",
        "      while \"ROOT\" != currentHead:\n",
        "        for k, word in enumerate(sentenceCopy):\n",
        "          if word._text == currentHead and word._id == currentHeadId and currentHeadId != currentNounId:\n",
        "            path.insert(0, currentHead + \" (\" + str(currentHeadId) + \")\")\n",
        "            currentNoun = word._text\n",
        "            currentNounId = word._id\n",
        "            currentHead = word._head\n",
        "            currentHeadId = word._headId\n",
        "\n",
        "    paths.append(path)\n",
        "\n",
        "  listed = []\n",
        "  for n1, noun1 in enumerate(nouns):\n",
        "    for n2, noun2 in enumerate(nouns):\n",
        "      if n1 != n2 and (noun1._text + noun2._text) not in listed and (noun2._text + noun1._text) not in listed:\n",
        "        path1 = paths[n1]\n",
        "        path2 = paths[n2]\n",
        "        output = \"\"\n",
        "\n",
        "        listed.append((noun1._text + noun2._text))\n",
        "        listed.append((noun2._text + noun1._text))\n",
        "\n",
        "        nn1 = noun1._text + \" (\"+ str(noun1._id) + \")\"\n",
        "        nn2 = noun2._text + \" (\"+ str(noun2._id) + \")\"\n",
        "\n",
        "        \n",
        "\n",
        "        if nn1 in path2:\n",
        "          output = \" -> \".join(path2[path2.index(nn1):path2.index(nn2)+1])\n",
        "        elif nn2 in path1:\n",
        "          output = \" -> \".join(path1[path1.index(nn2):path1.index(nn1)+1])\n",
        "        else:\n",
        "          bridge = None\n",
        "          path = []\n",
        "\n",
        "          path1copy = path1.copy()\n",
        "          path2copy = path2.copy()\n",
        "\n",
        "          flag = True\n",
        "          while path1copy[0] == path2copy[0]:\n",
        "            bridge = path1copy[0]\n",
        "            path1copy.pop(0)\n",
        "            path2copy.pop(0)\n",
        "          \n",
        "          path.append(bridge)\n",
        "\n",
        "          path = path + path1[path1.index(bridge)+1:path1.index(nn1)+1]\n",
        "          path.reverse()\n",
        "          output = \" <- \".join(path) + \" -> \"\n",
        "          output =  output + \" -> \".join(path2[path2.index(bridge)+1:path2.index(nn2)+1])\n",
        "        \n",
        "        print(noun1._text, \"-\", noun2._text,\": \", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tONiwtPdIQC5",
        "outputId": "d124f4da-eb37-46ad-abd7-03a3d476b366"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== sentence 1 nouns: ['AI', 'textbooks', 'field', 'study', 'agents', 'device', 'environment', 'actions', 'chance', 'goals'] ===\n",
            "\n",
            "AI - textbooks :  textbooks (3) -> AI (2)\n",
            "AI - field :  AI (2) <- textbooks (3) <- define (4) -> field (6)\n",
            "AI - study :  AI (2) <- textbooks (3) <- define (4) -> study (9)\n",
            "AI - agents :  AI (2) <- textbooks (3) <- define (4) -> study (9) -> agents (13)\n",
            "AI - device :  AI (2) <- textbooks (3) <- define (4) -> study (9) -> agents (13) -> device (17)\n",
            "AI - environment :  AI (2) <- textbooks (3) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> environment (21)\n",
            "AI - actions :  AI (2) <- textbooks (3) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24)\n",
            "AI - chance :  AI (2) <- textbooks (3) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28)\n",
            "AI - goals :  AI (2) <- textbooks (3) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28) -> achieving (31) -> goals (33)\n",
            "textbooks - field :  textbooks (3) <- define (4) -> field (6)\n",
            "textbooks - study :  textbooks (3) <- define (4) -> study (9)\n",
            "textbooks - agents :  textbooks (3) <- define (4) -> study (9) -> agents (13)\n",
            "textbooks - device :  textbooks (3) <- define (4) -> study (9) -> agents (13) -> device (17)\n",
            "textbooks - environment :  textbooks (3) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> environment (21)\n",
            "textbooks - actions :  textbooks (3) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24)\n",
            "textbooks - chance :  textbooks (3) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28)\n",
            "textbooks - goals :  textbooks (3) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28) -> achieving (31) -> goals (33)\n",
            "field - study :  field (6) <- define (4) -> study (9)\n",
            "field - agents :  field (6) <- define (4) -> study (9) -> agents (13)\n",
            "field - device :  field (6) <- define (4) -> study (9) -> agents (13) -> device (17)\n",
            "field - environment :  field (6) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> environment (21)\n",
            "field - actions :  field (6) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24)\n",
            "field - chance :  field (6) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28)\n",
            "field - goals :  field (6) <- define (4) -> study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28) -> achieving (31) -> goals (33)\n",
            "study - agents :  study (9) -> agents (13)\n",
            "study - device :  study (9) -> agents (13) -> device (17)\n",
            "study - environment :  study (9) -> agents (13) -> device (17) -> perceives (19) -> environment (21)\n",
            "study - actions :  study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24)\n",
            "study - chance :  study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28)\n",
            "study - goals :  study (9) -> agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28) -> achieving (31) -> goals (33)\n",
            "agents - device :  agents (13) -> device (17)\n",
            "agents - environment :  agents (13) -> device (17) -> perceives (19) -> environment (21)\n",
            "agents - actions :  agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24)\n",
            "agents - chance :  agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28)\n",
            "agents - goals :  agents (13) -> device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28) -> achieving (31) -> goals (33)\n",
            "device - environment :  device (17) -> perceives (19) -> environment (21)\n",
            "device - actions :  device (17) -> perceives (19) -> takes (23) -> actions (24)\n",
            "device - chance :  device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28)\n",
            "device - goals :  device (17) -> perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28) -> achieving (31) -> goals (33)\n",
            "environment - actions :  environment (21) <- perceives (19) -> takes (23) -> actions (24)\n",
            "environment - chance :  environment (21) <- perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28)\n",
            "environment - goals :  environment (21) <- perceives (19) -> takes (23) -> actions (24) -> maximize (26) -> chance (28) -> achieving (31) -> goals (33)\n",
            "actions - chance :  actions (24) -> maximize (26) -> chance (28)\n",
            "actions - goals :  actions (24) -> maximize (26) -> chance (28) -> achieving (31) -> goals (33)\n",
            "chance - goals :  chance (28) -> achieving (31) -> goals (33)\n"
          ]
        }
      ]
    }
  ]
}